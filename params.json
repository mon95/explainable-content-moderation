{"name":"Explainable Content Moderation of Offensive Images Using Convolutional Neural Networks","tagline":"CS6476 - Computer Vision Project","body":"### Introduction\r\n\r\nOne of the biggest challenges faced by social media networks today is content moderation. Users are free to upload text, images and videos that are easily visible and accessible by millions of other users. This becomes a problem when offensive content with gore, violence, sexually explicit images are posted. While there are currently human content reviewers, the volume of data produced automatically makes this a problem that needs to be solved at scale and thus is a good candidate to be solved using learning techniques. A general problem with content moderation is that any robust automation in the pipeline is unable to clearly demarcate the reason for flagging of a certain content. Any classification needs to indicate exactly why a certain content was flagged as inappropriate. This is important as we would like to be able to provide feedback to the user uploading the content as well as to potentially any human moderators auditing the moderation results to help prevent cases where images get mislabelled as offensive.\r\n\r\n\r\n### Goals and Objectives\r\nThe main objective of our project is to robustly classify and flag images as offensive or not. These images can be offensive due to the presence of violence, blood, weapons, sexual or other graphic content. Our model should be able to discriminate between such inappropriate images and other images.  \r\n\r\nThe classification decision made by the system should be explainable to ensure that the system does not harbour any biases such as flagging all images with humans as offensive or tagging all images with red liquids as offensive. We will highlight the salient regions that contribute most to the decision taken by the system. This will help us understand if the model has learned the discriminating factor accurately. It will help identify the regions when the model mislabels an image as offensive, such as objects that are shaped like guns or knives. The input to our system will be images and the output would be a binary classification labelling the images as proper or offensive.\r\n\r\n\r\n### System Architecture\r\n\r\n![](https://imgur.com/a/C94JWtt)\r\n\r\n### Approach\r\nWe will approach Content Moderation as a binary classification problem with 2 broad categories - offensive images and non-offensive images. \r\n\r\nWe propose to use Convolutional Neural Networks (CNNs) to perform a supervised binary classification of images into these two categories. We will be fine-tuning an existing CNN using transfer learning approaches to suit our domain. \r\n\r\nWe intend to use Grad-CAM[x] (cite), which is a class-discriminative localization technique to generate visual explanations of the salience regions in the classified images that caused the images to be classified as offensive. \r\n\r\nIn performing the classification and generating visual explanations, we build what would form the basis (a minimum viable product version) of a content moderation system for offensive images. The system could help end-users / human moderators identify flagged images as well as ‘see’ what caused the image to be flagged as offensive. In further experiments, we aim to explore how augmenting an image and/or replacing certain parts of the image with specific offensive/non-offensive content changes the behaviour.\r\n\r\nWe expect that there are latent factors involved in identifying an image as potentially offensive. For instance, in the case of differentiating between images portraying violence and other images, we may not always be able to look for objects (such as knives), or something specific like fire/blood, etc in the scene in a manner that is scalable. Classification of the images using a fine-tuned Convolutional Neural Network will help us identify these nuances involved in flagging the images and with the approach proposed, we should be able to understand and see the reasoning behind the same as well. We believe that such a model is more helpful in a real-world setting where there’s usually a combination of human content-reviewers, rule-based systems and ML systems working together and there’s a gap in understanding the reasons behind the machine’s take on a certain piece of content vs that of a human. \r\n\r\n### Experiments and Results\r\n\r\n#### Datasets\r\n1. Kaggle video dataset: https://www.kaggle.com/mohamedmustafa/real-life-violence-situations-dataset \r\nThis dataset contains a balanced mix of both violent and non-graphic videos from Youtube. The real-life violent content comes majorly from street fights. Apart from the violence videos, there are videos containing regular activities like eating and playing football. We intend to extract frames from each video and treat them as separate, labelled images. This wide range of activities in the dataset would help us make our model robust and take care of corner cases and false positives. \r\n\r\n2. Violent Scenes Dataset: https://www.interdigital.com/data_sets/violent-scenes-dataset#\r\nThis dataset is derived from the video clips of violent films. Apart from physical fights, the scenes in the clips include the presence of blood, fights, fire, guns,cold arms, car chases and other gory images. We found this dataset to be suitable because it encompasses different aspects of violence, in varying intensities and amounts, which would make our model explain a wide range of salient violence features. We have contacted the owners of this dataset and have been allowed by them to use it for our project. \r\n\r\n\r\n#### Measure of Success\r\n\r\nThe upfront measure of the success of our project is being able to classify violent images as offensive and being able to clearly point to the localized region in the image which caused the image to be classified as such. A further measure of success is being able to classify scenes with different types of violent activities (from fist-fights to gory accident scenes) and varying degrees of intensities of violence (two people fighting to mob violence). A crucial measure of success of our project would be being able to detect and explain only necessary and sufficient salient violence features (as few false positives as possible).We want the model to learn context-specific appropriateness for images too. A knife being present in the image of a kitchen should not be classified as offensive. \r\n\r\n#### Experiments\r\n\r\n| Experiment                                                                                                                        | Motivation Behind the Experiment                                                                        | Expected Result                                                   | Uncertainties in Results/Process                                             |\r\n|-----------------------------------------------------------------------------------------------------------------------------------|---------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------|------------------------------------------------------------------------------|\r\n| Replacing salient regions detected by  Grad-CAM with non-offensive objects   (eg. replacing a gun in an image with flower pot) | Check if the classification made depends on the objects only or also the context of the scene         | Image should now be classified as non-offensive | Is replacing one salient object in the image enough?                        |\r\n| Testing the model with  degrees of violence  (eg. replacing one swordsman  with multiple swordsmen)                               | How does the  classification/explanation  change if the replacement  is (relatively)  less/more violent | Images should correctly get  labeled according to  their context  | Model may not pick up heavily  suppressed instances of  (potential violence) |\r\n| Using an augmented  dataset to train model                                                                                        | Improving the accuracy of our model                                                                     | Higher precision and accuracy                                     | Obtaining/annotating an  augmented dataset?                                  |\r\n| Replacing salient regions  in violent images with  a blurred version                                                              | Automatically filter out  potential offensive images.                                                   | Image should be classified as  non-offensive                      |                                                                              |\r\n\r\n### Extensions to our Approach:\r\n\r\n1. Replacing salient regions detected by Grad-CAM with non-offensive objects and seeing if the classification output changes. This is to check if the classification made depends on the objects only or also the context of the scene .\r\n\r\n2. Data Augmentation by adding offensive objects to non-graphic images and checking if the classification of the scene changes.\r\n\r\n### Authors and Contributors\r\nTeam - PlacesVC\r\n","note":"Don't delete this file! It's used internally to help with page regeneration."}