{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import time\n",
    "import copy\n",
    "import sklearn.metrics\n",
    "\n",
    "'''\n",
    "File: classfier.py\n",
    "Project: cs6476-computervision-project\n",
    "File Created: October 2019\n",
    "Author: Shalini Chaudhuri\n",
    "'''\n",
    "class Classifier:\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "        Model Name: will be used to define which pretrained model we want to use\n",
    "        Feature Extract: Are we using the CNN as a feature extractor(changing only the final layer)\n",
    "                        or retraining for our problem\n",
    "        Num Epochs:\n",
    "        Batch Size:\n",
    "        OutPut Class: Binary classification, so 2\n",
    "    \"\"\"\n",
    "\n",
    "    model_name = None\n",
    "    output_classes = 2\n",
    "\n",
    "    def __init__(self, model_name, output_classes = 2, batch_size = 8, num_epochs=15, feature_extract=True):\n",
    "        self.model_name = model_name\n",
    "        self.output_classes = output_classes\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.feature_extract = feature_extract\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    \n",
    "    def train_model(self, model, criterion, optimizer, dataloaders, dataset_sizes, is_inception=False):\n",
    "        since = time.time()\n",
    "\n",
    "        best_model_weights = copy.deepcopy(model.state_dict())\n",
    "        best_accuracy = 0.0\n",
    "        val_acc_history = []\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "            print('Epoch {}/{}'.format(epoch, self.num_epochs - 1))\n",
    "            print('-' * 10)\n",
    "\n",
    "            # Each epoch has a training and validation phase\n",
    "            for phase in ['train', 'val']:\n",
    "                if phase == 'train':\n",
    "                    model.train()  # Set model to training mode\n",
    "                else:\n",
    "                    model.eval()   # Set model to evaluate mode\n",
    "\n",
    "                running_loss = 0.0\n",
    "                running_corrects = 0\n",
    "\n",
    "                # Iterate over data.\n",
    "                for inputs, labels in dataloaders[phase]:\n",
    "                    inputs = inputs.to(self.device)\n",
    "                    labels = labels.to(self.device)\n",
    "\n",
    "                    # zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # forward\n",
    "                    # track history if only in train\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        # outputs = model(inputs)\n",
    "                        # loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        if is_inception and phase == 'train':\n",
    "                            # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
    "                            outputs, aux_outputs = model(inputs)\n",
    "                            loss1 = criterion(outputs, labels)\n",
    "                            loss2 = criterion(aux_outputs, labels)\n",
    "                            loss = loss1 + 0.4*loss2\n",
    "                        else:\n",
    "                            outputs = model(inputs)\n",
    "                            loss = criterion(outputs, labels)\n",
    "\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                        # backward + optimize only if in training phase\n",
    "                        if phase == 'train':\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                    # statistics\n",
    "                    running_loss += loss.item() * inputs.size(0)\n",
    "                    running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "                    # if phase == 'train':\n",
    "                    #     scheduler.step()\n",
    "\n",
    "                epoch_loss = running_loss / dataset_sizes[phase]\n",
    "                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "                print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                    phase, epoch_loss, epoch_acc))\n",
    "\n",
    "                # deep copy the model\n",
    "                if phase == 'val' and epoch_acc > best_accuracy:\n",
    "                    best_accuracy = epoch_acc\n",
    "                    best_model_weights = copy.deepcopy(model.state_dict())\n",
    "                if phase == 'val':\n",
    "                    val_acc_history.append(epoch_acc)\n",
    "\n",
    "            print()\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "        print('Best val Acc: {:4f}'.format(best_accuracy))\n",
    "\n",
    "        # load best model weights\n",
    "        model.load_state_dict(best_model_weights)\n",
    "        return model, val_acc_history\n",
    "\n",
    "\n",
    "    def set_requires_grad(self, model):\n",
    "        if self.feature_extract:\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    def initPretrainedModel(self, inputSize):\n",
    "        model = None\n",
    "        input_size = 0\n",
    "        if self.model_name == 'alexnet' and self.feature_extract:\n",
    "            model = torchvision.models.alexnet(pretrained=True)\n",
    "            self.set_requires_grad(model)\n",
    "            num_ftrs = model.classifier[6].in_features\n",
    "            model.classifier[6] = nn.Linear(num_ftrs, self.output_classes)\n",
    "            input_size = inputSize\n",
    "\n",
    "        if self.model_name == 'inception' and self.feature_extract:\n",
    "            print(\"Initializing model: Inception_V3\")\n",
    "            model = models.inception_v3(pretrained=True)\n",
    "            self.set_requires_grad(model)\n",
    "            # Handle the auxilary net\n",
    "            num_ftrs = model.AuxLogits.fc.in_features\n",
    "            model.AuxLogits.fc = nn.Linear(num_ftrs, self.output_classes)\n",
    "            # Handle the primary net\n",
    "            num_ftrs = model.fc.in_features\n",
    "            model.fc = nn.Linear(num_ftrs, self.output_classes)\n",
    "            input_size = inputSize\n",
    "\n",
    "        if self.model_name == \"resnet\":\n",
    "            \"\"\"Resnet 18\n",
    "            \"\"\"\n",
    "            print(\"Initializing to use pre-trained Resnet 18 for feature extraction...\")\n",
    "            model = models.resnet18(pretrained=True)\n",
    "            self.set_requires_grad(model)\n",
    "            num_ftrs = model.fc.in_features\n",
    "            model.fc = nn.Linear(num_ftrs, self.output_classes)\n",
    "            input_size = inputSize\n",
    "\n",
    "        return model\n",
    "\n",
    "    def testModel(self, dataloaders, model, classes, dataset_sizes, batch_size):\n",
    "        correct = 0\n",
    "        total = dataset_sizes['test']\n",
    "        predictions = []\n",
    "\n",
    "        y_actual = []\n",
    "        y_pred = []\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for index, (inputs, labels) in enumerate(dataloaders['test'], 0):\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                \n",
    "                \n",
    "                samples = dataloaders['test'].dataset.samples[index*batch_size : index*batch_size + batch_size]\n",
    "                predicted_classes = [classes[predicted[j]] for j in range(predicted.size()[0])]\n",
    "                sample_names = [s[0] for s in samples]\n",
    "                \n",
    "                predictions.extend(list(zip(sample_names, predicted_classes)))\n",
    "\n",
    "                y_actual.extend(labels.numpy())\n",
    "                y_pred.extend(predicted.numpy())\n",
    "\n",
    "        try:\n",
    "            print(f\"Accuracy (Sklearn): {sklearn.metrics.accuracy_score(y_actual, y_pred)}\")\n",
    "            print(f\"F1-Score (Sklearn): {sklearn.metrics.f1_score(y_actual, y_pred)}\")\n",
    "            print(f\"Precision Score: {sklearn.metrics.precision_score(y_actual, y_pred)}\")\n",
    "            print(f\"Recall Score: {sklearn.metrics.recall_score(y_actual, y_pred)}\")\n",
    "            print(f\"\\nConfusion Matrix:\\n{sklearn.metrics.confusion_matrix(y_actual, y_pred)}\")\n",
    "            print(f\"\\nClassification Report:\\n{sklearn.metrics.classification_report(y_actual, y_pred)}\")\n",
    "        except RuntimeError:\n",
    "            print(\"Error computing metrics: \\n\", RuntimeError)\n",
    "\n",
    "        print('\\n\\nAccuracy of the network on the test images: %d %%' % (100 * correct / total))\n",
    "\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "'''\n",
    "File: dataset.py\n",
    "Project: cs6476-computervision-project\n",
    "File Created: October 2019\n",
    "Author: Shalini Chaudhuri (you@you.you)\n",
    "'''\n",
    "\n",
    "class DataSet:\n",
    "\n",
    "    \"\"\"\n",
    "        author: @chaudhsh\n",
    "\n",
    "        Ensure that the data directory looks like:\n",
    "\n",
    "        /data/violent/abc.jpg\n",
    "        /data/violent/abc1.jpg    \n",
    "\n",
    "        /data/nonviolent/xyz.jpg\n",
    "        /data/nonviolent/xyz1.jpg\n",
    "\n",
    "        This class will take care of automatically generating the dataset and labels for you.\n",
    "        labels will be \"violent\", \"nonviolent\"\n",
    "\n",
    "        1. initDataLoader will create the dataDictonary\n",
    "        2. setuploaderTransforms will ensure the inputs are resized to teh inputsize expected by\n",
    "        the pretrained network and normalized and converted to a tensor.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    data_dir = None\n",
    "\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "\n",
    "    @staticmethod\n",
    "    def initDataLoaders(data_dir, batch_size):\n",
    "        data_transforms = DataSet.setUpDataLoaderTransformers()\n",
    "        image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n",
    "                                          data_transforms[x])\n",
    "                  for x in ['train', 'val', 'test']}\n",
    "        dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size,\n",
    "                                                    shuffle=True, num_workers=4)\n",
    "                    for x in ['train', 'val', 'test']}\n",
    "        dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val', 'test']}\n",
    "        class_names = image_datasets['train'].classes\n",
    "\n",
    "\n",
    "        return dataloaders, dataset_sizes, class_names\n",
    "\n",
    "    @staticmethod\n",
    "    def setUpDataLoaderTransformers(inputSize = 224):\n",
    "                \n",
    "        data_transforms = {\n",
    "            'train': transforms.Compose([\n",
    "                transforms.RandomResizedCrop(inputSize),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            'val': transforms.Compose([\n",
    "                transforms.Resize(inputSize),\n",
    "                transforms.CenterCrop(inputSize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "            'test': transforms.Compose([\n",
    "                transforms.Resize(inputSize),\n",
    "                transforms.CenterCrop(inputSize),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "            ]),\n",
    "        }\n",
    "    \n",
    "        # data_transforms = {\n",
    "        #     'train': transforms.Compose([\n",
    "        #         transforms.Resize((inputSize,inputSize)),\n",
    "        #         transforms.ToTensor(),\n",
    "        #         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        #     ]),\n",
    "        #     'val': transforms.Compose([\n",
    "        #         transforms.Resize((inputSize,inputSize)),\n",
    "        #         transforms.ToTensor(),\n",
    "        #         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        #     ]),\n",
    "        #     'test': transforms.Compose([\n",
    "        #         transforms.Resize((224,224)),\n",
    "        #         transforms.ToTensor(),\n",
    "        #         transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "        #     ]),\n",
    "        # }\n",
    "\n",
    "        return data_transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch.optim as optim\n",
    "\n",
    "'''\n",
    "File: optimizer.py\n",
    "Project: cs6476-computervision-project\n",
    "File Created: October 2019\n",
    "Author: Shalini Chaudhuri (you@you.you)\n",
    "'''\n",
    "\n",
    "class Optimizer:\n",
    "\n",
    "    \"\"\"\n",
    "        author: @chaudhsh\n",
    "\n",
    "        Creates an stochastic gradient descent optmizer. and checks if it needs to backprop\n",
    "        the gradients.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, device):\n",
    "        self.device = device\n",
    "\n",
    "    def optimize(self, model, feature_extract, learningRate, momentum):\n",
    "        model = model.to(self.device)\n",
    "        params_to_update = model.parameters()\n",
    "        print(\"Params to learn:\")\n",
    "        if feature_extract:\n",
    "            params_to_update = []\n",
    "            for name,param in model.named_parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    params_to_update.append(param)\n",
    "                    print(\"\\t\", name)\n",
    "        else:\n",
    "            for name, param in model.named_parameters():\n",
    "                if param.requires_grad == True:\n",
    "                    print(\"\\t\", name)\n",
    "\n",
    "        # optimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n",
    "        optimizer_ft = optim.SGD(params_to_update, lr=learningRate, momentum=momentum)\n",
    "\n",
    "        return optimizer_ft\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  1.3.0\n",
      "C:\\Users\\rohit\\Documents\\GitHub\\cs6476-computervision-project\\data\\train\n",
      "Params to learn:\n",
      "\t classifier.6.weight\n",
      "\t classifier.6.bias\n",
      "Epoch 0/14\n",
      "----------\n",
      "train Loss: 0.5591 Acc: 0.7877\n",
      "val Loss: 0.3359 Acc: 0.8950\n",
      "\n",
      "Epoch 1/14\n",
      "----------\n",
      "train Loss: 0.5904 Acc: 0.8019\n",
      "val Loss: 0.3085 Acc: 0.8980\n",
      "\n",
      "Epoch 2/14\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-cde789ef1129>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0moptimizer_ft\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \u001b[0mdataloaders_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 51\u001b[1;33m     dataset_sizes,is_inception=False)\n\u001b[0m\u001b[0;32m     52\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m torch.save({\n",
      "\u001b[1;32m~\\Documents\\GitHub\\cs6476-computervision-project\\code\\classifier.py\u001b[0m in \u001b[0;36mtrain_model\u001b[1;34m(self, model, criterion, optimizer, dataloaders, dataset_sizes, is_inception)\u001b[0m\n\u001b[0;32m     82\u001b[0m                             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m0.4\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\vera\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\vera\\lib\\site-packages\\torchvision\\models\\vgg.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\vera\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\vera\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\vera\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    540\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 541\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    542\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\vera\\lib\\site-packages\\torch\\nn\\modules\\batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     79\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrack_running_stats\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             exponential_average_factor, self.eps)\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\vera\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   1668\u001b[0m     return torch.batch_norm(\n\u001b[0;32m   1669\u001b[0m         \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1670\u001b[1;33m         \u001b[0mtraining\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1671\u001b[0m     )\n\u001b[0;32m   1672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "from optimizer import Optimizer\n",
    "from classifier import Classifier\n",
    "from dataset import DataSet\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "\n",
    "\n",
    "data_dir = \"C:\\\\Users\\\\rohit\\\\Documents\\\\GitHub\\\\cs6476-computervision-project\\\\data\"\n",
    "model_name = \"vgg\"\n",
    "num_classes = 2\n",
    "batch_size = 4\n",
    "num_epochs = 15\n",
    "feature_extract = True\n",
    "\n",
    "vggClassifier = Classifier(model_name, num_classes)\n",
    "model = vggClassifier.initPretrainedModel(224)\n",
    "\n",
    "dataloaders, dataset_sizes, class_names = DataSet.initDataLoaders(data_dir, batch_size)\n",
    "data_transforms = DataSet.setUpDataLoaderTransformers(inputSize = 224)\n",
    "\n",
    "print(os.path.join(data_dir, 'train'))\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x]) for x in ['train', 'val']}\n",
    "dataloaders_dict = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=batch_size, shuffle=True, num_workers=4) for x in ['train', 'val']}\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sgdOptimizer = Optimizer(device)\n",
    "optimizer_ft = sgdOptimizer.optimize(model, feature_extract, 0.001, 0.9)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "model, hist = vggClassifier.train_model(model, \n",
    "    criterion, \n",
    "    optimizer_ft, \n",
    "    dataloaders_dict, \n",
    "    dataset_sizes,is_inception=False)\n",
    "\n",
    "torch.save({\n",
    "'name': 'vggFeatureExtraction',\n",
    "'epoch': 15,\n",
    "'model_state_dict': model.state_dict(),\n",
    "'optimizer_state_dict': optimizer_ft.state_dict(),\n",
    "}, 'C:\\\\Users\\\\rohit\\\\Documents\\\\GitHub\\\\cs6476-computervision-project\\\\trainedModels\\\\vggFeatureExtraction.pt')\n",
    "\n",
    "'''ohist = [h.cpu().numpy() for h in hist]\n",
    "\n",
    "plt.title(\"Validation Accuracy vs. Number of Training Epochs\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.plot(range(1,num_epochs+1),ohist,label=\"Pretrained\")\n",
    "plt.ylim((0,1.))\n",
    "plt.xticks(np.arange(1, num_epochs+1, 1.0))\n",
    "plt.legend()\n",
    "plt.show()'''\n",
    "'''\n",
    "# testing\n",
    "state = torch.load('C:\\\\Users\\\\rohit\\\\Documents\\\\GitHub\\\\cs6476-computervision-project\\\\trainedModels\\\\vggFeatureExtraction.pt')\n",
    "model.load_state_dict(state['model_state_dict'])\n",
    "predictions = vggClassifier.testModel(dataloaders_dict, model, class_names, dataset_sizes, batch_size=4)\n",
    "\n",
    "# save predicted values\n",
    "np.savetxt('C:\\\\Users\\\\rohit\\\\Documents\\\\GitHub\\\\cs6476-computervision-project\\\\trainedModels\\\\trainedModels\\\\predictedLabelsVgg.csv', predictions, fmt='%s')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
